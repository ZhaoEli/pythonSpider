import re
import csv
from fake_useragent import UserAgent
import  time
import requests
import random
import datetime
from lxml import etree
from lxml.html import fromstring,tostring
'''
    äºŒçº§é™æ€é¡µé¢æŠ“å–
    
    S1ï¼šè·å–é¡µé¢pageé“¾æ¥ï¼Œå¹¶æ‹¼åˆURL
    S2ï¼šè¿›å…¥é“¾æ¥é¡µé¢è§£æé¡µé¢
    S3ï¼šè£…å¡«é¡µé¢
    
'''
class housing_Prices_Spider(object):
    def __init__(self):
        #è®¾ç½®å†™å…¥çš„æ–‡ä»¶åï¼šè´å£³æˆ¿ä»·é‡‡é›†_2022.08.03
        self.filename = './hangzhou_price/è´å£³æˆ¿ä»·é‡‡é›†_'+str(datetime.date.today())

        #ä¸€çº§é¡µé¢çš„url
        self.one_url='https://hz.fang.ke.com/loupan/pg{page_number}'
        
        #è®¾å®šä¸€ä¸ªæµè§ˆå™¨ä»£ç†å°é©¬ç”²
        self.headers = {'User-Agent': UserAgent().chrome}

        #äºŒçº§é¡µé¢éœ€è¦æ‹¼åˆä¸€ä¸‹å‰ç¼€ï¼Œæ‰èƒ½æ­£å¸¸é“¾æ¥
        self.prefix_url='https://hz.fang.ke.com'
        #æ¥¼ç›˜åœ°å€
        self.re_bds_url = ".//div[@class='resblock-name']/a[@class='name ']/@href"

    def get_List(self, m_url, headers,re_bds):
        html1=requests.get(url=m_url,headers=headers).text
        html1=html1.replace('\n\t\t\t','')
        parse_html = etree.HTML(html1)
        # åŸºå‡† xpath è¡¨è¾¾å¼ï¼Œurlçš„èŠ‚ç‚¹å¯¹è±¡
        # url_list = parse_html.xpath(self.re_bds_name)
        # list=url_list[0].attrib
        # print(list['href'])
        url_list = parse_html.xpath(re_bds)
        return url_list
    def get_realValue(self,parse_html,xpath_bds):
        li = parse_html.xpath(xpath_bds)  # æŠ“å–æ•°æ®
        #å¦‚æœæŠ“å–ä¸åˆ°æ•°æ®ï¼Œliä¸ºç©ºè¡¨ï¼Œåˆ™èµ‹å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå¦‚æœæŠ“å–åˆ°åˆ™æå–
        li = li[0].strip() if li else ''
        return li


    def run(self,min_page_number,max_page_number):
        #æ‰“å¼€æ–‡ä»¶å†™å…¥ç«¯å£
        filename=self.filename+'.csv'
        f =open(filename, 'a', newline='', encoding='utf-8')
        w = csv.writer(f,delimiter=',')
        #è®¾ç½®rebdsåŒ¹é…è§„åˆ™
        area=".//a[@class='resblock-location']/text()" #æ¥¼ç›˜æ‰€å¤„æ¿å—
        name=".//div/h2[@class='DATA-PROJECT-NAME']/text()"
        other_name=".//div[@class='other-name']/text()"
        price=".//div[@class='top-info ']/div[@class='price']/span[@class='price-number']/text() | .//div[@class='info-wrap']/div[@class='top-info ']/div[@class='price']/span[@class='price-unit']/text()"
        # unit=""
        # value=".//div[@class='top-info ']/div[@class='price']/span[@class='price-number'][2]/text()"
        # value_unit = ".//div[@class='top-info ']/div[@class='price']/span[@class='price-unit'][1]/text()"
        tag_item=".//div[@class='title-wrap']/div[1]/div[@class='tags-wrap']/span[@class='tag-item sell-type-tag']/text()"
        house_type=".//div[@class='title-wrap']/div[1]/div[@class='tags-wrap']/span[@class='tag-item house-type-tag']/text()"
        address=".//div[@class='resblock-info animation qr-fixed post_ulog_exposure_scroll']/div[@class='info-wrap']/div[@class='middle-info animation']/ul[@class='info-list']/li[@class='info-item'][1]/span[@class='content']/text()"
        new_open_date=".//div[@class='resblock-info animation qr-fixed post_ulog_exposure_scroll']/div[@class='info-wrap']/div[@class='middle-info animation']/ul[@class='info-list']/li[@class='info-item open-date-wrap']/div[@class='open-date']/span[@class='content']/text()"
        rooms=".//div[@class='resblock-info animation qr-fixed post_ulog_exposure_scroll']/div[@class='info-wrap']/div[@class='middle-info animation']/ul[@class='info-list']/li[@class='info-item'][2]/span[@class='content']/span[@class='house-type-item']/text()"
        collecting_time=str(datetime.date.today()) #æ•°æ®é‡‡é›†æ—¶é—´
        feature =".//span[@class='label-val tese-val']/text()"
        builders = ".//ul[@class='x-box'][1]/li[@class='all-row'][3]/span[@class='label-val']/text()"

        for page_number in range(min_page_number,max_page_number ):

            one_url=self.one_url.format(page_number=page_number)
            one_url_list=self.get_List(one_url,self.headers,self.re_bds_url)
            one_area_list=self.get_List(one_url,self.headers,area)

            for i in range(0,len(one_url_list)):
                # è·å–æ¥¼ç›˜é¡µé¢ä¿¡æ¯
                two_url=self.prefix_url+one_url_list[i]
                html1 = requests.get(url=two_url, headers=self.headers).text
                parse_html = etree.HTML(html1)

                t_name = self.get_realValue(parse_html,name) #æ¥¼ç›˜å
                print(t_name)
                t_other_name = self.get_realValue(parse_html, other_name) #åˆ«å
                t_other_name= t_other_name.replace('åˆ«åï¼š','')

                t_price= parse_html.xpath(price) #å•ä»·
                # t_unit = parse_html.xpath(unit) #å•ä»·å•ä½
                # t_value = parse_html.xpath(price)[1].strip() # æ€»ä»·
                # t_value_unit = parse_html.xpath(unit)[1].strip() #æ€»ä»·å•ä½
                t_sale_status = self.get_realValue(parse_html, tag_item) #æ˜¯å¦åœ¨å”®
                t_house_type = self.get_realValue(parse_html, house_type) #æˆ¿å±‹ç±»å‹ ä½å®… å•†ç”¨ å•†ä½ä¸¤ç”¨
                t_address = self.get_realValue(parse_html, address) #åœ°å€
                t_new_open_date = self.get_realValue(parse_html, new_open_date) #æœ€æ–°å¼€ç›˜
                if t_new_open_date=='' :
                    t_new_open_date = self.get_realValue(parse_html,".//div[@class='resblock-info animation qr-fixed post_ulog_exposure_scroll']/div[@class='info-wrap']/div[@class='middle-info animation']/ul[@class='info-list']/li[@class='info-item'][2]/span[@class='content']/text()")
                t_rooms = parse_html.xpath(rooms) #å‡ å®¤
                t_areas=one_area_list[i].strip()
                # è·å–"æ›´å¤š"è¯¦æƒ…é¡µ
                html = requests.get(url=two_url + "xiangqing/", headers=self.headers).text
                parse_html = etree.HTML(html)
                t_feature= self.get_realValue(parse_html, feature) #ç‰¹è‰²æè¿°ï¼šæˆç†Ÿå•†åœˆ å“ç‰Œæˆ¿ä¼ VRçœ‹æˆ¿ ç»¿åŒ–ç‡é«˜
                t_builders = self.get_realValue(parse_html, builders) #å¼€å‘å•†

                # æ‹¼å­—æ®µæˆä¸€ä¸ªlistï¼Œæ–¹ä¾¿æŒ‰è¡Œå†™å…¥
                dict = {}
                dict.update({'name':t_name})
                dict.update({'collecting_time': collecting_time})
                dict.update({'other_name':t_other_name})
                dict.update({'area':t_areas})
                if len(t_price) ==4: #priceæ˜¯å•ç‹¬çš„æ•°ç»„
                    dict.update({'price':t_price[0]})
                    dict.update({'uint': t_price[1]})
                    dict.update({'value': t_price[2]})
                    dict.update({'value_uint': t_price[3]})
                elif len(t_price) ==2:
                    dict.update({'price':t_price[0]})
                    dict.update({'uint': t_price[1]})
                else: dict.update({'price':''})

                dict.update({'sale_status':t_sale_status})
                dict.update({'house_type':t_house_type})
                dict.update({'address':t_address})
                dict.update({'new_open_date':t_new_open_date})
                dict.update({'rooms':t_rooms if t_rooms else ''})
                dict.update({'features':t_feature})
                dict.update({'builders': t_builders})
                dict.update({'url_source': two_url})#urlåœ°å€
                w.writerow([dict])
                time.sleep(random.randint(1, 2))
            print('è·å–å®Œæˆç¬¬{d}ä¸ªé¡µé¢â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”overâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”'.format(d=page_number))
        f.close()


    # def run(self):
    #     for page_number in range(self.min_page_number,self.max_page_number+1):
    #         one_url=self.one_url.format(page_number=page_number)
    #         html = self.get_html(m_url=one_url, headers=self.headers)
    #         #è·å–ä¸€çº§é¡µé¢å†…å®¹çš„é“¾æ¥ï¼Œå‡†å¤‡åšè·³è½¬åˆ°äºŒçº§
    #         one_list = self.parse_html(self.re_bds_one, html)
    #         #è·å–äºŒçº§è¯¦æƒ…é¡µå¹¶å†™å…¥csv
    #         self.second_list_to_write(self.prefix_url,self.re_bds_two,one_list)
    #         print('è·å–å®Œæˆç¬¬{d}ä¸ªé¡µé¢â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”overâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”'.format(d=page_number))
    #     print('ok,ç¨‹åºè¿è¡Œå®Œæ¯•ï¼ŒæŠ“å–äº†{page_one}~{page_two}é¡µçš„æ•°æ®ã€‚'.format(page_one=self.min_page_number,page_two=self.max_page_number))


#ä¸»å‡½æ•°
if __name__ == '__main__':

    # éœ€è¦çˆ¬å–ç½‘é¡µçš„èŒƒå›´
    min_page_number = 60 #ä»å½“å‰é¡µå¼€å§‹é‡‡é›†
    max_page_number = 81 #ç»ˆæ­¢é¡µé¢ï¼Œé‡‡é›†æ•°æ®ä¸åŒ…æ‹¬å½“å‰é¡µã€‚æ³¨æ„ï¼ï¼ğŸ“¢

    try:
        spider = housing_Prices_Spider()
        spider.run(min_page_number,max_page_number)
    except Exception as e:
        print(e)
